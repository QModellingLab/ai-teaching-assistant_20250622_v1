# utils.py - é…é¡æ„ŸçŸ¥çš„æ™ºæ…§å‚™æ¡ˆç³»çµ±

import os
import json
import datetime
import logging
import time
import google.generativeai as genai
from models import Student, Message, Analysis, db

logger = logging.getLogger(__name__)

# åˆå§‹åŒ– Gemini AI - æ™ºæ…§é…é¡ç®¡ç†
GEMINI_API_KEY = os.getenv('GEMINI_API_KEY')
model = None
current_model_name = None

# é…é¡è¿½è¹¤å­—å…¸
quota_tracker = {
    'gemini-2.0-flash': {'daily_limit': 200, 'used_today': 0, 'last_reset': None},
    'gemini-2.0-flash-lite': {'daily_limit': 200, 'used_today': 0, 'last_reset': None},
    'gemini-1.5-flash': {'daily_limit': 1500, 'used_today': 0, 'last_reset': None},  # 1.5 ç‰ˆæœ¬é€šå¸¸æœ‰æ›´é«˜é™åˆ¶
    'gemini-1.5-pro': {'daily_limit': 50, 'used_today': 0, 'last_reset': None},
}

def reset_daily_quota_if_needed():
    """æª¢æŸ¥æ˜¯å¦éœ€è¦é‡ç½®æ¯æ—¥é…é¡è¨ˆæ•¸"""
    today = datetime.date.today()
    
    for model_name in quota_tracker:
        tracker = quota_tracker[model_name]
        if tracker['last_reset'] != today:
            tracker['used_today'] = 0
            tracker['last_reset'] = today
            logger.info(f"ğŸ”„ é‡ç½® {model_name} æ¯æ—¥é…é¡è¨ˆæ•¸")

def is_model_available(model_name):
    """æª¢æŸ¥æ¨¡å‹æ˜¯å¦é‚„æœ‰é…é¡å¯ç”¨"""
    reset_daily_quota_if_needed()
    
    if model_name not in quota_tracker:
        return True  # æœªçŸ¥æ¨¡å‹ï¼Œå‡è¨­å¯ç”¨
    
    tracker = quota_tracker[model_name]
    available = tracker['used_today'] < tracker['daily_limit']
    
    if not available:
        logger.warning(f"âš ï¸ {model_name} é…é¡å·²ç”¨å®Œ ({tracker['used_today']}/{tracker['daily_limit']})")
    
    return available

def record_model_usage(model_name):
    """è¨˜éŒ„æ¨¡å‹ä½¿ç”¨æ¬¡æ•¸"""
    if model_name in quota_tracker:
        quota_tracker[model_name]['used_today'] += 1
        used = quota_tracker[model_name]['used_today']
        limit = quota_tracker[model_name]['daily_limit']
        logger.info(f"ğŸ“Š {model_name} ä½¿ç”¨é‡: {used}/{limit}")

if GEMINI_API_KEY:
    try:
        genai.configure(api_key=GEMINI_API_KEY)
        
        # æ™ºæ…§æ¨¡å‹é¸æ“‡ - ä¾é…é¡å¯ç”¨æ€§æ’åº
        models_to_try = [
            # å„ªå…ˆä½¿ç”¨é‚„æœ‰é…é¡çš„æ¨¡å‹
            'gemini-2.0-flash-lite',     # æ‚¨é‚„æœ‰ 36% é…é¡å¯ç”¨
            'gemini-1.5-flash',          # é€šå¸¸é…é¡æ›´é«˜
            'gemini-1.5-flash-001',      
            'gemini-1.5-flash-8b',       # 8B ç‰ˆæœ¬æ›´ç¶“æ¿Ÿ
            'gemini-1.5-pro',            # åŠŸèƒ½æ›´å¼·ä½†é…é¡è¼ƒå°‘
            'gemini-1.5-pro-001',
            'gemini-2.0-flash',          # å·²è¶…é™ï¼Œä½†å¯èƒ½åŠå¤œé‡ç½®
            'gemini-2.0-flash-001',      
            'gemini-1.0-pro',            # æœ€å¾Œå‚™æ¡ˆ
            'gemini-1.0-pro-001'
        ]
        
        for model_name in models_to_try:
            # æª¢æŸ¥é…é¡ç‹€æ…‹
            if not is_model_available(model_name):
                logger.info(f"â­ï¸ è·³é {model_name} (é…é¡ä¸è¶³)")
                continue
                
            try:
                test_model = genai.GenerativeModel(model_name)
                
                # è¼•é‡æ¸¬è©¦ï¼ˆé¿å…æµªè²»é…é¡ï¼‰
                test_response = test_model.generate_content(
                    "Test",
                    generation_config=genai.types.GenerationConfig(
                        max_output_tokens=5,
                        temperature=0.1
                    )
                )
                
                if test_response and test_response.text:
                    model = test_model
                    current_model_name = model_name
                    record_model_usage(model_name)  # è¨˜éŒ„æ¸¬è©¦ä½¿ç”¨
                    logger.info(f"âœ… Gemini AI æˆåŠŸåˆå§‹åŒ–ï¼Œä½¿ç”¨æ¨¡å‹: {model_name}")
                    break
                    
            except Exception as e:
                error_msg = str(e)
                if "429" in error_msg:
                    logger.warning(f"âš ï¸ {model_name} é…é¡è¶…é™ï¼Œæ¨™è¨˜ç‚ºä¸å¯ç”¨")
                    # æ¨™è¨˜ç‚ºå·²ç”¨å®Œ
                    if model_name in quota_tracker:
                        quota_tracker[model_name]['used_today'] = quota_tracker[model_name]['daily_limit']
                elif "403" in error_msg:
                    logger.warning(f"âš ï¸ {model_name} æ¬Šé™ä¸è¶³: {error_msg[:50]}")
                elif "404" in error_msg:
                    logger.warning(f"âš ï¸ {model_name} æ¨¡å‹ä¸å­˜åœ¨")
                else:
                    logger.warning(f"âš ï¸ {model_name} æ¸¬è©¦å¤±æ•—: {error_msg[:50]}")
                continue
        
        if not model:
            logger.error("âŒ æ‰€æœ‰ Gemini æ¨¡å‹éƒ½ä¸å¯ç”¨")
            logger.info("ğŸ’¡ å»ºè­°è§£æ±ºæ–¹æ¡ˆ:")
            logger.info("   1. ç­‰å¾…é…é¡é‡ç½®ï¼ˆé€šå¸¸ç‚º UTC åˆå¤œï¼‰")
            logger.info("   2. å‡ç´šåˆ°ä»˜è²»æ–¹æ¡ˆä»¥ç²å¾—æ›´é«˜é…é¡")
            logger.info("   3. æ–°å¢å…¶ä»– AI æœå‹™å•†ä½œç‚ºå‚™æ¡ˆ")
            
    except Exception as e:
        logger.error(f"âŒ Gemini AI åˆå§‹åŒ–å¤±æ•—: {e}")
else:
    logger.warning("âš ï¸ Gemini API key not found")

def generate_ai_response_with_smart_fallback(student_id, query, conversation_context="", student_context="", group_id=None):
    """æ™ºæ…§å‚™æ¡ˆçš„ AI å›æ‡‰ç”Ÿæˆ"""
    global model, current_model_name
    
    if not model:
        logger.error("âŒ AI æ¨¡å‹æœªåˆå§‹åŒ–")
        return "Sorry, AI service is currently unavailable. This might be due to daily quota limits. Please try again later."
    
    try:
        # æª¢æŸ¥ç•¶å‰æ¨¡å‹æ˜¯å¦é‚„æœ‰é…é¡
        if not is_model_available(current_model_name):
            logger.warning(f"âš ï¸ ç•¶å‰æ¨¡å‹ {current_model_name} é…é¡ä¸è¶³ï¼Œå˜—è©¦åˆ‡æ›...")
            
            # å˜—è©¦åˆ‡æ›åˆ°å…¶ä»–å¯ç”¨æ¨¡å‹
            success = switch_to_available_model()
            if not success:
                return "AI service has reached daily quota limits. Please try again later or contact administrator to upgrade the service plan."
        
        # æ§‹å»ºæç¤ºè©
        student = Student.get_by_id(student_id) if student_id else None
        
        prompt = f"""You are an AI Teaching Assistant for English-medium instruction (EMI) courses.

{"Previous conversation context:" + chr(10) + conversation_context + chr(10) if conversation_context else ""}

Instructions:
- Respond primarily in clear, simple English suitable for university-level ESL learners
- Use vocabulary appropriate for intermediate English learners
- For technical terms, provide Chinese translation in parentheses when helpful
- Maintain a friendly, encouraging, and educational tone
- If this continues a previous conversation, build on what was discussed before
- Encourage further questions and deeper thinking

{student_context if student_context else ""}

Student question: {query}

Please provide a helpful response (100-150 words):"""
        
        logger.info(f"ğŸ¤– ä½¿ç”¨ {current_model_name} ç”Ÿæˆå›æ‡‰...")
        
        # æ ¹æ“šæ¨¡å‹èª¿æ•´é…ç½®
        if 'lite' in current_model_name or '8b' in current_model_name:
            # è¼•é‡æ¨¡å‹ä½¿ç”¨è¼ƒä¿å®ˆçš„è¨­å®š
            generation_config = genai.types.GenerationConfig(
                candidate_count=1,
                max_output_tokens=300,
                temperature=0.6,
                top_p=0.8,
                top_k=30
            )
        else:
            # æ¨™æº–æ¨¡å‹è¨­å®š
            generation_config = genai.types.GenerationConfig(
                candidate_count=1,
                max_output_tokens=350,
                temperature=0.7,
                top_p=0.9,
                top_k=40
            )
        
        response = model.generate_content(prompt, generation_config=generation_config)
        
        if response and response.text:
            # è¨˜éŒ„æˆåŠŸä½¿ç”¨
            record_model_usage(current_model_name)
            
            ai_response = response.text.strip()
            logger.info(f"âœ… AI å›æ‡‰æˆåŠŸç”Ÿæˆï¼Œé•·åº¦: {len(ai_response)} å­—")
            return ai_response
        else:
            logger.error("âŒ AI å›æ‡‰ç‚ºç©º")
            return "Sorry, I cannot generate an appropriate response right now. Please try again later."
            
    except Exception as e:
        error_msg = str(e).lower()
        logger.error(f"âŒ AI å›æ‡‰éŒ¯èª¤: {str(e)}")
        
        # æ™ºæ…§éŒ¯èª¤è™•ç†
        if "429" in error_msg or "quota" in error_msg or "limit" in error_msg:
            # é…é¡å•é¡Œï¼Œå˜—è©¦åˆ‡æ›æ¨¡å‹
            logger.warning("ğŸ”„ åµæ¸¬åˆ°é…é¡å•é¡Œï¼Œå˜—è©¦åˆ‡æ›æ¨¡å‹...")
            success = switch_to_available_model()
            if success:
                logger.info(f"âœ… å·²åˆ‡æ›åˆ° {current_model_name}ï¼Œé‡æ–°å˜—è©¦...")
                # éè¿´é‡è©¦ä¸€æ¬¡
                return generate_ai_response_with_smart_fallback(student_id, query, conversation_context, student_context, group_id)
            else:
                return "AI service quota exceeded. Please try again later when quota resets (typically at midnight UTC)."
        else:
            return "An error occurred while processing your question. Please try again later."

def switch_to_available_model():
    """åˆ‡æ›åˆ°å¯ç”¨çš„æ¨¡å‹"""
    global model, current_model_name
    
    models_to_try = [
        'gemini-2.0-flash-lite',
        'gemini-1.5-flash', 
        'gemini-1.5-flash-8b',
        'gemini-1.5-pro',
        'gemini-2.0-flash',
        'gemini-1.0-pro'
    ]
    
    for model_name in models_to_try:
        if model_name == current_model_name:
            continue  # è·³éç•¶å‰æ¨¡å‹
            
        if not is_model_available(model_name):
            continue
            
        try:
            test_model = genai.GenerativeModel(model_name)
            # å¿«é€Ÿæ¸¬è©¦
            test_response = test_model.generate_content(
                "Hi",
                generation_config=genai.types.GenerationConfig(max_output_tokens=3)
            )
            
            if test_response and test_response.text:
                model = test_model
                current_model_name = model_name
                record_model_usage(model_name)
                logger.info(f"âœ… æˆåŠŸåˆ‡æ›åˆ° {model_name}")
                return True
                
        except Exception as e:
            if "429" in str(e):
                # æ¨™è¨˜ç‚ºé…é¡ç”¨å®Œ
                if model_name in quota_tracker:
                    quota_tracker[model_name]['used_today'] = quota_tracker[model_name]['daily_limit']
            continue
    
    logger.error("âŒ æ²’æœ‰å¯ç”¨çš„å‚™æ¡ˆæ¨¡å‹")
    return False

def get_quota_status():
    """å–å¾—é…é¡ç‹€æ…‹å ±å‘Š"""
    reset_daily_quota_if_needed()
    
    status = {
        'current_model': current_model_name,
        'models': {},
        'recommendations': []
    }
    
    for model_name, tracker in quota_tracker.items():
        used_pct = (tracker['used_today'] / tracker['daily_limit']) * 100 if tracker['daily_limit'] > 0 else 0
        status['models'][model_name] = {
            'used': tracker['used_today'],
            'limit': tracker['daily_limit'],
            'available': tracker['daily_limit'] - tracker['used_today'],
            'usage_percent': round(used_pct, 1),
            'status': 'å¯ç”¨' if used_pct < 100 else 'å·²ç”¨å®Œ'
        }
    
    # ç”Ÿæˆå»ºè­°
    available_models = [name for name, info in status['models'].items() if info['usage_percent'] < 100]
    if not available_models:
        status['recommendations'].append("æ‰€æœ‰æ¨¡å‹é…é¡å·²ç”¨å®Œï¼Œå»ºè­°ç­‰å¾…é‡ç½®æˆ–å‡ç´šæ–¹æ¡ˆ")
    elif len(available_models) == 1:
        status['recommendations'].append(f"åƒ…å‰© {available_models[0]} å¯ç”¨ï¼Œå»ºè­°ç¯€ç´„ä½¿ç”¨")
    
    return status

def test_ai_connection():
    """æ¸¬è©¦ AI é€£æ¥"""
    try:
        if not model:
            return False, "AI æ¨¡å‹æœªåˆå§‹åŒ–"
        
        quota_status = get_quota_status()
        available_models = [name for name, info in quota_status['models'].items() if info['usage_percent'] < 100]
        
        if not available_models:
            return False, "æ‰€æœ‰æ¨¡å‹é…é¡å·²ç”¨å®Œ"
        
        return True, f"é€£æ¥æ­£å¸¸ - ç•¶å‰: {current_model_name}, å¯ç”¨æ¨¡å‹: {len(available_models)}"
        
    except Exception as e:
        return False, f"é€£æ¥éŒ¯èª¤: {str(e)[:60]}..."

# å…¼å®¹æ€§ï¼šä¿æŒåŸæœ‰å‡½æ•¸åç¨±
def generate_ai_response(student_id, query, conversation_context="", student_context="", group_id=None):
    """åŸæœ‰å‡½æ•¸çš„å…¼å®¹æ€§åŒ…è£"""
    return generate_ai_response_with_smart_fallback(student_id, query, conversation_context, student_context, group_id)
